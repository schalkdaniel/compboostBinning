---
title: Binning Numerical Features
author: Daniel S.
output:
  prettydoc::html_pretty:
    theme: tactile
---

```{r, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

# devtools::load_all("..")
```


# References

- [Zheyuan Li, Simon N. Wood: "Faster model matrix crossproducts for large generalized linear models with discretized covariates"](https://link.springer.com/content/pdf/10.1007/s11222-019-09864-2.pdf)
- [Lang et al.: "Multilevel structured additive regression"](https://link.springer.com/article/10.1007/s11222-012-9366-0)
    - First formulation of _binning_ (page 8, 3.3. remark 2)
- [Simon N. Wood,  Zheyuan Li, Gavin  Shaddick, and Nicole H.  Augustin: "Generalized Additive Models for Gigadata: Modeling the U.K. Black Smoke Network Daily Data"](https://amstat.tandfonline.com/doi/full/10.1080/01621459.2016.1195744#.XoHEQClfiWg)
    - Argumentation that $\sqrt{n}$ unique points are sufficient.

# Goal

"___Reducing memory load and runtime performance by shorten the number of observations used for fitting a linear model.___"


# Methodology

## Faster model matrix crossproducts with discretized covariates

### General idea

The linear models requires the computation of the matrix $X^TWX$ where $X\in\mathbb{R}^{n\times p}$ is the design matrix and $W$ a weight matrix. We restrict our scenario here to just diagonal weight matrices and $X$ as base of a one feature. This does not necessarily mean that $X\in\mathbb{R}^{1\times n}$ but that $X$ is created as basis of a feature vector $x\in \mathbb{R}^n$ (for instance spline basis).

The core idea of Simon Wood and Zheyuan Li is to create another matrix $\bar{X}\in\mathbb{R}^{\bar{n}\times p}$ and $\bar{W}\in\mathbb{R}^{\bar{n}\times\bar{n}}$ based on the unique values of $x$ so that the following property holds:
$$
X^TWX = \bar{X}^T\bar{W}\bar{X}
$$

### Index vector for mapping

The mapping of $\bar{X}$ to $X$ is defined by an index vector $k$, such that $X(i,.) = \bar{X}(k(i), .)$, for example:
$$
X = \left(\begin{array}{c}
3.1 \\ 1.2 \\ 2.4 \\ 1.2 \\ 1.2 \\ 2.4 \\ 3.1
\end{array}\right) \qquad \bar{X} = \left(\begin{array}{c}
3.1 \\ 1.2 \\ 2.4
\end{array}\right)\qquad k = \left(\begin{array}{c}
1 \\ 2 \\ 3 \\ 2 \\ 2 \\ 3 \\ 1
\end{array}\right)
$$

### Algorithm to compute crossproduct

Again, we assume that $W$ has diagonal form $W = \text{diag}(w)$. The following algorithm explains how to get from $w$ to $\bar{w}$ where $\bar{W} = \text{diag}(\bar{w})$. The algorithm is an adaption of Algorithm 3 from "Zheyuan Li, Simon N. Wood" by using $\bar{A} = \bar{B} = \bar{X}$. The idea is to first calculate $D = \bar{X}^T\bar{W}$ and then $D^T\bar{X}$:

1. Set $D^T = O_{p\times \bar{n}}$
1. For $i = 1, \dots, n$ do $D(,i) \mathrel{{+}{=}} w(i)\bar{X}(i,)$
1. Return $D^T\bar{X}$

Instead of calculating $D$ we calculate directly $D^T$ to not do another transposing step.

Another adaption to reduce the number of vector additions (for diagonal $W$) is to accumulate the weights and do one matrix multiplication in the end:

1. Set $w_c = \vec{0} \in \mathbb{R}^{\bar{n}}$
1. For $i = 1, \dots, n$ do $w_c \mathrel{{+}{=}} w(i)$
1. Return $(X \otimes w_c)^T X$


#### Microbenchmark

For small runtime comparison we simulate $n = 10^6$ x values that are used for cubic spline regression of degree 16 (20 parameter). The number of unique values are $\sqrt{n}$:
M

```{r}
nsim = 1e6L
nunique = trunc(sqrt(nsim))

# Sample data:
xunique = runif(n = nunique, min = 0, max = 10)
k = sample(x = seq_len(nunique), size = nsim, replace = TRUE)

y = 2 * sin(xunique) + rnorm(nsim, 0, 0.5)

# Calculate knots of given x values:
knots = compboostSplines::createKnots(values = xunique, n_knots = 16, degree = 3)

# Create basis using that knots:
X = compboostSplines::createSplineBasis(values = xunique, degree = 3, knots = knots)
# We assume matrix to be stored transposed (this is the case in compboost):
X_sparse = Matrix::t(compboostSplines::createSparseSplineBasis(values = xunique, degree = 3, knots = knots))
X_full = X[k,]


# Define C++ matrix multiplication for comparison:
code_mat_mult = "
arma::mat cppMatMult (const arma::mat& X) {
  return arma::trans(X) * X;
}
"
code_mat_mult_weight = "
arma::mat cppMatMultWeight (const arma::mat& X, const arma::vec& w) {
  return arma::trans(X.each_col() % w) * X;
}
"

Rcpp::cppFunction(code = code_mat_mult, depends = "RcppArmadillo")
Rcpp::cppFunction(code = code_mat_mult_weight, depends = "RcppArmadillo")
```

First we look at the case without weights. Here, the weights are initialized as 1. __Note__ The cpp matrix multiplication is not very fair since the conversion of the R matrix to C++ is also not for free:
```{r}
# We have equal weights in this case:
w = 1

xtx_full = t(X_full) %*% X_full

xtx = binnedMatMult(X = X, k = k-1, w = w, FALSE)
all.equal(xtx_full, xtx, check.attributes = FALSE)

xtx = binnedMatMult(X = X, k = k-1, w = w, TRUE)
all.equal(xtx_full, xtx, check.attributes = FALSE)

xtx_sp = binnedSparseMatMult(X = X_sparse, k = k-1, w = w)
xtx_sp[1:10, 1:10]
xtx_full[1:10, 1:10]

microbenchmark::microbenchmark(
  "R mat mult" = { t(X_full) %*% X_full },
  "C++ mat mult" = { cppMatMult(X_full) },
  "C++ binned mat mult" = { binnedMatMult(X = X, k = k-1, w = w, FALSE) },
  "C++ binned mat mult fast" = { binnedMatMult(X = X, k = k-1, w = w, TRUE) },
  "C++ binned mat mult fast sparse" = { binnedSparseMatMult(X = X_sparse, k = k-1, w = w) },
  times = 10L
)
```

The next case is the case with weights:
```{r}
# With weights:
w = sample(x = seq_len(20L), size = nsim, replace = TRUE)

xtwx_full = t(X_full * w)  %*% X_full

xtwx = binnedMatMult(X = X, k = k-1, w = w, FALSE)
all.equal(xtwx_full, xtwx, check.attributes = FALSE)

xtwx = binnedMatMult(X = X, k = k-1, w = w, TRUE)
all.equal(xtwx_full, xtwx, check.attributes = FALSE)

microbenchmark::microbenchmark(
  "R mat mult" = { t(X_full * w) %*% X_full },
  "C++ mat mult" = { cppMatMultWeight(X_full, w) },
  "C++ binned mat mult" = { binnedMatMult(X = X, k = k-1, w = w, FALSE) },
  "C++ binned mat mult fast" = { binnedMatMult(X = X, k = k-1, w = w, TRUE) },
  "C++ binned mat mult fast sparse" = { binnedSparseMatMult(X = X_sparse, k = k-1, w = w) },
  times = 10L
)
```

#### Adaptions

- Sum up weights and then multiply columns with cumulated weights `mat mult fast`

### Compute matrix product w.r.t. response

The aim is to use binning for component-wise boosting. The problem here is, that the matrix $X^TX$ is computet once and then reused over and over again. But the part $X^Ty$ has to be calculated in every iteration over and over again. Therefore, we are more interested in computing $u = X^TWy$ more efficient. The adaption to the upper algorithm is very easy:

1. Set $u = \vec{0}$
1. For $i = 1, \dots, n$ do $u \mathrel{{+}{=}} w(i)y(i)\bar{X}(i,)$
1. Return $u$

Small adaption to make the algorithm even faster (for diagonal $W$). This reduces the $n$ vector addition in every step to one matrix multiplication with $\bar{n}$ vector additions:

1. Set $w_c = \vec{0} \in \mathbb{R}^{\bar{n}}$
1. For $i = 1, \dots, n$ do $w_c\mathrel{{+}{=}} w(i)y(i)$
1. Return $\bar{X} w_c$


#### Microbenchmark

```{r}
out = binnedMatMultResponse(X = X, y = y, k = k-1, w = 1)
out_full = t(X_full) %*% y

all.equal(out, out_full, check.attributes = FALSE)

out_sp = binnedSparseMatMultResponse(X = X_sparse, y = y, k = k-1, w = 1)
out_sp[1:10]
out_full[1:10]


microbenchmark::microbenchmark(
  "R mat mult" = { t(X_full) %*% y },
  "C++ binned mat mult" = { binnedMatMultResponse(X = X, y = y, k = k-1, w = 1) },
  "C++ binned mat mult sparse" = { binnedSparseMatMultResponse(X = X_sparse, y = y, k = k-1, w = 1) },
  times = 10L
)


out = binnedMatMultResponse(X = X, y = y, k = k-1, w = w)
out_full = t(X_full * w) %*% y

all.equal(out, out_full, check.attributes = FALSE)

out_sp = binnedSparseMatMultResponse(X = X_sparse, y = y, k = k-1, w = w)
out_sp[1:10]
out_full[1:10]

microbenchmark::microbenchmark(
  "R mat mult" = { t(X_full * w) %*% y },
  "C++ binned mat mult" = { binnedMatMultResponse(X = X, y = y, k = k-1, w = w) },
  "C++ binned mat mult sparse" = { binnedSparseMatMultResponse(X = X_sparse, y = y, k = k-1, w = w) },
  times = 10L
)
```


### Weighted least squares microbenchmark


```{r}
# Fit weighted least squares:
beta = binnedMatMultResponse(X = X, y = y, k = k-1, w = w) %*% solve(binnedMatMult(X = X, k = k-1, w = w))
beta_full = solve(t(X_full * w) %*% X_full) %*% t(X_full * w) %*% y

all.equal(t(beta), beta_full, check.attributes = FALSE)

microbenchmark::microbenchmark(
  "Weighted OLS R" = { solve(t(X_full * w) %*% X_full) %*% t(X_full * w) %*% y },
  "Weighted OLS binned" = { binnedMatMultResponse(X = X, y = y, k = k-1, w = w) %*% solve(binnedMatMult(X = X, k = k-1, w = w)) },
  "Weighted OLS binned sparse" = { solve(binnedSparseMatMult(X = X_sparse, k = k-1, w = w)) %*% binnedSparseMatMultResponse(X = X_sparse, y = y, k = k-1, w = w) },
  times = 5L
)
```

## Binning numerical features

The idea here is to discretize the vector $x\in\mathbb{R}^n$ to a vector $\bar{x}$ where the number of unique values $\text{unique}(\bar{x}) = m_u < n_u = \text{unique}(x)$. The vector $\bar{x}$ is then used for faster model matrix crossproducts as explained [above](#faster-model-matrix-crossproducts-with-discretized-covariates).

In "Generalized Additive Models for Gigadata: Modeling the U.K. Black Smoke Network Daily Data" they say to use a equidistant grid of $m_u = \sqrt{n}$ points.

1. Calculate $x_1^\ast = \text{min}(x)$ and $x_{m_u}^\ast = \text{max}(x)$ with $m_u = \lfloor\sqrt{n}\rfloor$
1. Calculate $x_1^\ast,\dots, x_{m_u}^\ast$ and set $\delta = x_2^\ast - x_1^\ast$
1. Calculate vector of indizes $k(i)$ (here `x_bins`$= x^\ast$):
```
for (i = 1, ..., n) do
| while (x_bins(i) + delta < x(i))
| | j += 1;
| endwhile
| k(i) = j
endfor
```




